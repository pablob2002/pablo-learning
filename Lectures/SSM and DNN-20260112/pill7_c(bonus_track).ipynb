{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy VS Tensorflow Neural Network\n",
    "\n",
    "In this excersis you will build a neural network from scratch using Numpy. For a sake of simplicity, we will not use a OOP approach, instead we will create the NN with simple functions.\n",
    "\n",
    "Follow the instructions and descriptions for each function and let's create a NN from scratch witn numpy. Then, we will build the same NN with the tensorflow library to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "features = data[\"features\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with numpy \n",
    "\n",
    "We will implement all the functions necessary to train a fully connected NN using only the numpy library. The objective is to be able to train a neural network with *any number of layers* in which the last layer will have a **single neuron** with a **sigmoid activation** function and the other layers any number of neurons with a **relu activation** function.\n",
    "\n",
    "The following figure shows a diagram of how we will implement the NN training process (*take your time to understand it!*):\n",
    "\n",
    "<img src=\"diag.png\" alt=\"Neural network training diagram\" style=\"height: 550px;\"/>\n",
    "\n",
    "The code will be *structured in basic functions* that are composed according to the following scheme:\n",
    "\n",
    "- L_layer_model\n",
    "  - initialize_parameters\n",
    "  - L_model_forward\n",
    "    - linear_activation_forward\n",
    "      - linear_forward\n",
    "      - sigmoid\n",
    "      - relu\n",
    "  - compute_cost\n",
    "  - L_model_backward\n",
    "    - linear_activation_backward\n",
    "      - linear_backward\n",
    "      - sigmoid_backward\n",
    "      - relu_backward\n",
    "  - update_parameters\n",
    "- accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**:\n",
    "- We denote $L$ the number of layers of the neural network.\n",
    "- We denote the weight matrix that connects one layer to the next with the letter $W$, whereas we denote the bias vector with the letter $b$.\n",
    "- Superscript $[l]$ denotes a quantity associated with layer number $l$.\n",
    "     - Example: $a^{[L]}$ denotes the output of layer number $L$.\n",
    "     - Example: The variables $W^{[L]}$ and $b^{[L]}$ denote the weight matrix and the bias vector that connect layer $L-1$ with layer $L$ respectively .\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i$-th example.\n",
    "     - Example: $x^{(i)}$ is the $i$-th element of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrices must be initialized using the normal distribution and the bias vectors must be initialized with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    layer_dims -- list with the dimension of each layer: e.g. [10,5,1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dic with parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector with shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "At each layer of a neural network, the neuron inputs are combined linearly before passing through the activation function according to the following formula:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     A -- output of previous layer (or input data): (number of neurons in previous layer, number of examples)\n",
    "     W -- weight matrix: (number of neurons in the current layer, number of neurons in the previous layer)\n",
    "     b -- bias vector: (number of neurons in the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the entry to the activation function\n",
    "    cache -- a triplet containing \"A\", \"W\", and \"b\", used later for backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the linear combination of the inputs of a layer has been calculated, a non-linear activation function must be applied before sending the outputs to the next layer. If we denote $g$ the activation function (in our case relu or sigmoid), we have the following formula:\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     Z -- output of linear forward\n",
    "     \n",
    "    Returns:\n",
    "     A -- g(Z), activation function value\n",
    "     cache -- Z, used later for backward propagation\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     Z -- output of linear forward\n",
    "     \n",
    "    Returns:\n",
    "     A -- g(Z), activation function value\n",
    "     cache -- Z, used later for backward propagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements forward propagation of a layer including the activation function\n",
    "\n",
    "    Inputs:\n",
    "    A_prev -- output of the previous layer (or input data):(number of neurons in the previous layer, number of examples)\n",
    "    W -- weight matrix: (number of neurons in the current layer, number of neurons in the previous layer)\n",
    "    b -- bias vector: (number of neurons in the current layer, 1)\n",
    "    activation -- the name of the activation function to use in the layer: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Outputs:\n",
    "    A -- the output of the layer after applying the activation function\n",
    "    cache -- a pair containing \"linear_cache\" and \"activation_cache\", then used for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":    \n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input data, the output of the NN is calculated by applying different layers one after another. If we denote the last layer as $L$, the output of the NN corresponds to the output of the last layer $A^{[L]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation of the entire neural network\n",
    "    \n",
    "    Inputs:\n",
    "    X -- data: size array (number of variables, number of examples)\n",
    "    parameters -- output of initialize_parameters() function\n",
    "    \n",
    "    Returns:\n",
    "    AL -- neural network output\n",
    "    caches -- list of caches containing all the caches of the linear_activation_forward() function, the caches\n",
    "                indexed from 0 to L-2 correspond to the relu activation function caches and the indexed cache\n",
    "                as L-1 corresponds to the cache of the sigmoid activation function\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    # deep layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # output layer\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Now, we can obtain a value that measures the performance of the NN using a cost function $\\mathcal{L}$. We will use the log-loss cost function, which is defined by the following formula:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Calculate the cost function\n",
    "\n",
    "    Inputs:\n",
    "    AL -- vector containing the output of the network, corresponding to the probabilities predicted by the neural network\n",
    "            for each example: (1, number of examples)\n",
    "    Y -- vector with the correct labels for the input data to the network: (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- value of the log-loss cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -1 * np.mean(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), (1 - Y)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "To train a neural network it is necessary to calculate the gradient of the cost function with respect to the network parameters, for which we will use backward propagation. Backpropagation consists of applying the chain rule to calculate the gradient of the cost function step by step in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the chain rule to the linear part of the neuron, suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l ]}}$. So, to calculate the derivatives $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ we can use the following formulas:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of backpropagation for a single layer\n",
    "\n",
    "    Inputs:\n",
    "    dZ -- derivative of the cost function with respect to the linear output of the current layer\n",
    "    cache -- triple containing the values (A_prev, W, b), coming from the linear_forward function\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- derivative of the cost function with respect to the output of the previous layer (l-1): has the same size as A_prev\n",
    "    dW -- derivative of the cost function with respect to the weight matrix W of the current layer (l): has the same size as W\n",
    "    db -- derivative of the cost function with respect to the bias vector b of the current layer (l): has the same size as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.mean(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply the chain rule to the nonlinear part of the neurons, that is, to the activation functions. For this, if we denote $g$ as the activation function, we can use the following formula:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}),$$\n",
    "\n",
    "where $*$ indicates the product component by component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     dA -- derivative of the cost function with respect to the output of the current layer (l)\n",
    "     cache -- \"activation_cache\", coming from the linear_activation_forward function\n",
    "     \n",
    "    Returns:\n",
    "     dZ -- derivative of the activation function\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     dA -- derivative of the cost function with respect to the output of the current layer (l)\n",
    "     cache -- \"activation_cache\", coming from the linear_activation_forward function\n",
    "     \n",
    "    Returns:\n",
    "     dZ -- derivative of the activation function\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements single layer backpropagation including activation function\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- derivative of the cost function with respect to the output of the current layer (l)\n",
    "    cache -- pair containing \"linear_cache\" and \"activation_cache\", coming from the linear_activation_forward function\n",
    "    activation -- the name of the activation function used in the current layer (l): \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Bring back:\n",
    "    dA_prev -- derivative of the cost function with respect to the output of the previous layer (l-1): has the same size as A_prev\n",
    "    dW -- derivative of the cost function with respect to the weight matrix W of the current layer (l): has the same size as W\n",
    "    db -- derivative of the cost function with respect to the bias vector b of the current layer (l): has the same size as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is possible to calculate the derivative of the cost function with respect to any of the parameters by applying the newly implemented functions starting with the last layer. Let's note that to initialize the back propagation it is necessary to first calculate the value of $\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement back propagation of the entire neural network\n",
    "    \n",
    "    Inputs:\n",
    "    AL -- neural network output, comes from the L_model_forward function\n",
    "    Y -- vector with the correct labels for each example in the data set: (1, number of examples)\n",
    "    caches -- list of caches containing all the caches of the linear_activation_forward() function, the caches\n",
    "                indexed from 0 to L-2 correspond to the relu activation function caches and the indexed cache\n",
    "                as L-1 corresponds to the cache of the sigmoid activation function\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the derivatives of the cost function with respect to each variable:\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # initialize the back propagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # last layer gradient\n",
    "    current_cache = linear_activation_backward(dAL, caches[L-1], \"sigmoid\")\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = current_cache\n",
    "\n",
    "    # deep layers gradient\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], \"relu\")\n",
    "        dA_prev_temp, dW_temp, db_temp = current_cache\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating parameters\n",
    "\n",
    "Once we have the gradient of the cost function we can use the **gradient descent method** to update the parameters of the neural network. If we denote $\\alpha$ the learning rate, the formulas to apply a gradient descent step are:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Inputs:\n",
    "    parameters -- dictionary containing the neural network parameters\n",
    "    grads -- dictionary with the derivatives of the cost function with respect to each parameter,\n",
    "                corresponds to the output of the L_model_backward function\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary with updated parameters:\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost):\n",
    "    \"\"\"\n",
    "    Implements a neural network of L layers where the first L-1 layers have activation function relu and\n",
    "    The last layer has sigmoid activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    X -- data: size array (number of variables, number of examples)\n",
    "    Y -- vector with the correct labels for each example in the data set: (1, number of examples)\n",
    "    layers_dims -- length list (number of layers + 1) containing the number of variables and the number\n",
    "                    of neurons in each layer,\n",
    "    learning_rate -- learning rate to apply the gradient descent method\n",
    "    num_iterations -- number of steps to apply gradient descent\n",
    "    print_cost -- if True, writes the value of the cost function every 10 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- adjusted neural network parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameter initialization\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward prop\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # cost function\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # backrpop\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # params update\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # printing the cost every 10 iterations\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.696913\n",
      "Cost after iteration 10: 0.691836\n",
      "Cost after iteration 20: 0.686587\n",
      "Cost after iteration 30: 0.676047\n",
      "Cost after iteration 40: 0.650420\n",
      "Cost after iteration 50: 0.590681\n",
      "Cost after iteration 60: 0.493065\n",
      "Cost after iteration 70: 0.405640\n",
      "Cost after iteration 80: 0.345785\n",
      "Cost after iteration 90: 0.302525\n",
      "Cost after iteration 100: 0.269174\n",
      "Cost after iteration 110: 0.242394\n",
      "Cost after iteration 120: 0.220268\n",
      "Cost after iteration 130: 0.201715\n",
      "Cost after iteration 140: 0.186070\n",
      "Cost after iteration 150: 0.172704\n",
      "Cost after iteration 160: 0.161199\n",
      "Cost after iteration 170: 0.151184\n",
      "Cost after iteration 180: 0.142391\n",
      "Cost after iteration 190: 0.134606\n",
      "Cost after iteration 200: 0.127648\n",
      "Cost after iteration 210: 0.121405\n",
      "Cost after iteration 220: 0.115761\n",
      "Cost after iteration 230: 0.110564\n",
      "Cost after iteration 240: 0.105818\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [100, 20, 5, 1]\n",
    "parameters = L_layer_model(train_x.T, train_y.reshape(1, -1), layers_dims=layers_dims, learning_rate=0.1, \n",
    "                           num_iterations=250, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the neural network's predictions.\n",
    "    \n",
    "    Inputs:\n",
    "    X -- data: size array (number of variables, number of examples)\n",
    "    parameters -- parameters of the trained neural network\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- value between 0 and 1 that represents the accuracy of the neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # forward prop\n",
    "    probs, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # ConversiÃ³n de la salida de la red a valores 0 o 1\n",
    "    for i in range(0, probs.shape[1]):\n",
    "        if probs[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "            \n",
    "    accuracy = np.sum((p == y)) / m\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy(test_x.T, test_y.reshape(1, -1), parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow and Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def keras_model(layers_dims, learning_rate):\n",
    "    \"\"\"\n",
    "    Create, using Keras or tensorflow, a neural network of L fully connected layers where the first L-1 layers\n",
    "    They have relu activation function and the last layer has sigmoid activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    layers_dims -- length list (number of layers + 1) containing the number of variables and the number\n",
    "                    of neurons in each layer,\n",
    "    learning_rate -- learning rate to apply the gradient descent method\n",
    "    \n",
    "    Returns:\n",
    "    model -- Keras object that represents the neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers_dims[1], input_shape=(layers_dims[0],), activation=\"relu\"))\n",
    "    \n",
    "    for l in range(2, L-1):\n",
    "        model.add(Dense(layers_dims[l], activation=\"relu\", kernel_initializer=\"random_normal\",\n",
    "                bias_initializer=\"zeros\"))\n",
    "    \n",
    "    model.add(Dense(layers_dims[L-1], activation=\"sigmoid\", kernel_initializer=\"random_normal\",\n",
    "                bias_initializer=\"zeros\"))\n",
    "\n",
    "    opt=SGD(learning_rate==learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.6913 - accuracy: 0.6125\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6894 - accuracy: 0.7394\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6860 - accuracy: 0.7894\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6790 - accuracy: 0.8338\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6645 - accuracy: 0.8731\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6328 - accuracy: 0.9187\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5626 - accuracy: 0.9413\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.9463\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2496 - accuracy: 0.9500\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9625\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9644\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9663\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9681\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0741 - accuracy: 0.9706\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9744\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.9756\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9762\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9781\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9787\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0527 - accuracy: 0.9800\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0505 - accuracy: 0.9825\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0486 - accuracy: 0.9825\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0468 - accuracy: 0.9837\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9844\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.9850\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0423 - accuracy: 0.9856\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9856\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9856\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9856\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9869\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9875\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 990us/step - loss: 0.0353 - accuracy: 0.9881\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9881\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.9881\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9881\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9894\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9900\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9900\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0288 - accuracy: 0.9900\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9900\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9900\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0266 - accuracy: 0.9900\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.9900\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9906\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9906\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9906\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9906\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9912\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9919\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.9925\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.9925\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9925\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.9931\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.9931\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9931\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.9931\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0183 - accuracy: 0.9931\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0179 - accuracy: 0.9937\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9937\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9944\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0158 - accuracy: 0.9944\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0154 - accuracy: 0.9944\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9956\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0147 - accuracy: 0.9956\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9956\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0137 - accuracy: 0.9962\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9962\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0127 - accuracy: 0.9962\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9969\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9969\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0119 - accuracy: 0.9969\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0113 - accuracy: 0.9975\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0111 - accuracy: 0.9975\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0108 - accuracy: 0.9975\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 0.9975\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0101 - accuracy: 0.9975\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.9975\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0097 - accuracy: 0.9975\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9975\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0093 - accuracy: 0.9981\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9987\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9987\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9987\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.9987\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9994\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9994\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9994\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0077 - accuracy: 0.9994\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0075 - accuracy: 0.9994\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d3778c6d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = [100, 20, 5, 1]\n",
    "model = keras_model(layers_dims = layers_dims, learning_rate = 0.1)\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=train_x.shape[0], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.983\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy {:.3f}\".format(model.evaluate(test_x, test_y, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style=\"border-radius:15px\">\n",
    "<b>EXCERCISE / TAKE HOME IDEAS:</b> <br>\n",
    "1) Compare the performance of both implementations. Use different hiperparameters such as:<br>\r",
    "- number of layers <br>\n",
    "- different dimension for each layer <br>\n",
    "- epochs <br>\n",
    "- etc <br>\n",
    "<br>\n",
    "2) Program the numpy NN from scratch using OOP.</div>div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
